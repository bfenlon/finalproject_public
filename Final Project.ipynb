{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bryce Fenlon\n",
    "\n",
    "## Final Assignment\n",
    "\n",
    "This notebook will serve to outline the the project and data used within the project. All private Twitter developer keys will be loaded from a separate file, and will not be included in the final submission. \n",
    "\n",
    "#### Project Description\n",
    "I have chosen to do this project on the ongoing social distancing and quarantine orders throughout the United States and around the world. Specifically, how these measures impact tweet sentiments. Several locations around the United States will be chosen to focus this study on, provided they fit certain criteria. Those criteria are:\n",
    "1. The area is or has recently (within the past week) been under strict stay-at-home orders or other similarly strict regulations\n",
    "2. Those stay-at-home orders or strict regulations are set to be eased (to any extent) before May 6, 2020.\n",
    "3. The tweet volume in the area chosen is sufficient to conduct statistical analysis with\n",
    "\n",
    "For this project, I will be going outside the scope of this course to the realm of machine learning. I had to rely heavily on other projects and techniques, but I have made an effort to fully understand what is going on. The comments should reflect this. Any code explicitly taken from other projects/repositories will be cited in the code comments or in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will start this notebook with the creation of a classification model.\n",
    "# The dataset used to create the model is the Sentiment140 set on Kaggle, a dataset with 1.6\n",
    "# tweets tagged for sentiment between 0 and 4 (0 = negative, 4 = positive).\n",
    "# Start by loading pandas and numpy to work with the data in the csv.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# >>>>>>>>>>>>> IMPORTANT <<<<<<<<<<<<< #\n",
    "# Switch following lines to 'True' to start model creation from scratch\n",
    "create_save_model = False\n",
    "\n",
    "# >>>>>>>>>>> ARE YOU SURE? <<<<<<<<<<< #\n",
    "create_save_model = False\n",
    "\n",
    "if create_save_model == True:\n",
    "    # Read the training data in, keep only the sentiment value and the text of the tweet\n",
    "    cols = ['sentiment', 'text']\n",
    "    training = pd.read_csv('training.1600000.processed.noemoticon.csv', usecols = [0, 5], header = None, names = cols, encoding = \"ISO-8859-1\")\n",
    "\n",
    "    # Though the description of the data hints at a neutral value, there are none present\n",
    "    # Because of this, we will simply reduce 4s to 1s in the sentiment value to make after double checking there are no 2s\n",
    "    # training easier in later steps\n",
    "    training = training[training.sentiment != 2]\n",
    "    training.sentiment = training.sentiment / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Create dictionary of negation contractions, not comprehensive\n",
    "neg_dict = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "# Create dictionary of 'positive' contractions, \n",
    "# not comprehensive and some are arbitrary choices, like \"he would\" instead of \"he had\"\n",
    "pos_dict = {\"could've\":\"could have\", \"he'd\":\"he would\", \"she'd\":\"she would\", \"he'll\":\"he will\",\n",
    "                \"she'll\":\"she will\", \"how'd\":\"how would\", \"how'll\":\"how will\", \"how're\":\"how are\",\n",
    "                \"how's\":\"how is\", \"i'd\":\"i would\", \"i'll\":\"i will\", \"i'm\":\"i am\", \"i've\":\"i have\",\n",
    "                \"it's\":\"it is\", \"let's\":\"let us\", \"might've\":\"might have\", \"must've\":\"must have\",\n",
    "                \"he's\":\"he is\", \"she's\":\"she is\", \"they're\":\"they are\", \"they'd\":\"they would\",\n",
    "                \"should've\":\"should have\", \"that's\":\"that is\", \"that'd\":\"that would\", \"there'll\":\"there will\",\n",
    "                \"there're\":\"there are\", \"they'll\":\"they will\", \"they've\":\"they have\", \"we'll\":\"we will\",\n",
    "                \"we'd\":\"we would\", \"why's\":\"why is\"}\n",
    "def clean_tweet(some_tweet):\n",
    "    # use regex to remove URLs, @s, and #s by matching on the character (or http) \n",
    "    # and replacing everything after it until a whitespace character ('\\S+')with an empty string\n",
    "    # let's not forget any old fashioned 'www' type links\n",
    "    text = re.sub(r'http\\S+', '', some_tweet).lower()\n",
    "    text = re.sub(r'www.\\S+', '', text)\n",
    "    text = re.sub(r'@\\S+', '', text)\n",
    "    text = re.sub(r'#\\S+', '', text)\n",
    "    text = text.replace('\\n', ' ') # remove new lines\n",
    "    for i in text.split():\n",
    "        if i.isdigit(): # remove numbers\n",
    "            text = text.replace(i, \"\")\n",
    "        # handle negative contractions with neg_dict\n",
    "        for k, v in neg_dict.items():\n",
    "            if i == k:\n",
    "                text = text.replace(k, v)\n",
    "        # handle positive contractions with pos_dict\n",
    "        for k, v in pos_dict.items():\n",
    "            if i == k:\n",
    "                text = text.replace(k, v)\n",
    "    for x in string.punctuation + \"‘’”“\": # eliminate punctuation and other special characters\n",
    "        text = text.replace(x, \" \")\n",
    "    # Eliminate non-ASCII characters (i.e. emoticons/emojis), because training data does not have emoticons\n",
    "    text = \"\".join(i for i in text if ord(i)<128)\n",
    "    # Get rid of words of single letters or extraneous single letters,\n",
    "    # we will rely on the rest of the text to produce our results,\n",
    "    # and we don't want random letters from ad hoc (e.g. ':P') emojis screwing up anything\n",
    "    text = \" \".join([x for x in text.split() if len(x) > 1])\n",
    "    # Return cleaned text of tweet\n",
    "    return(text)\n",
    "\n",
    "if create_save_model == True:\n",
    "    # Clean text of each tweet in training data\n",
    "    training.text = training.text.apply(clean_tweet)\n",
    "    x = training.text\n",
    "    y = training.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    # Clean text of each tweet in training data frame\n",
    "    # We're purposefully leaving stop words in, \n",
    "    training.text = training.text.apply(clean_tweet)\n",
    "    x = training.text\n",
    "    y = training.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    import sklearn\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # Split data into training and validation with sklearn's 'train_test_split'\n",
    "    x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creation\n",
    "\n",
    "Now begins the fun part. Using Tfidf vectorizer from `sklearn`, we will use 'Term frequency-inverse document frequency' to extract features for our neural network. Features are basically the input for a neural network, the machine readable stuff that allows us to train and adjust weights within a machine learning model. In this case, we will be extracting 100,000 features from the x_train data created in the previous step. Model creation and `batch_generator` function is taken from https://github.com/tthustla/twitter_sentiment_analysis_part9/blob/master/Capstone_part4-Copy7.ipynb. Final validation accuracy for the Sentiment140 data was 82.28%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    # Now we need to transform our text to something machine readable\n",
    "    # The result will be a sparse matrix (that will need to be converted to dense)\n",
    "\n",
    "    # This line initializes the Tfidf vectorizer with our given parameters\n",
    "    tvec = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "\n",
    "    # Here we iterate through the training data to count occurrences of those 100,000 features\n",
    "    tvec.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    # Transform text training and validation data, converting x_validation data to dense matrix with .toarray() method\n",
    "    # x_train data is simply too large, so the batch_generator function is needed (in the next cell)\n",
    "    x_train_tfidf = tvec.transform(x_train)\n",
    "    x_validation_tfidf = tvec.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    def batch_generator(X_data, y_data, batch_size):\n",
    "        samples_per_epoch = X_data.shape[0]\n",
    "        number_of_batches = samples_per_epoch/batch_size\n",
    "        counter=0\n",
    "        index = np.arange(np.shape(y_data)[0])\n",
    "        while 1:\n",
    "            index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "            X_batch = X_data[index_batch,:].toarray()\n",
    "            y_batch = y_data[y_data.index[index_batch]]\n",
    "            counter += 1\n",
    "            yield X_batch,y_batch\n",
    "            if (counter > number_of_batches):\n",
    "                counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Flatten\n",
    "    from keras.layers.embeddings import Embedding\n",
    "    from keras.preprocessing import sequence\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_dim=100000))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    model.fit_generator(generator = batch_generator(x_train_tfidf, y_train, 64),\n",
    "                epochs=2, \n",
    "                validation_data=(x_validation_tfidf, y_validation),\n",
    "                steps_per_epoch=x_train_tfidf.shape[0]/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if create_save_model == True:\n",
    "    model_json = model.to_json()\n",
    "    with open('model.json', 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    model.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and sentiment tagging of tweets\n",
    "All the Twitter data from our two time periods will now be loaded, the text collected and cleaned, and then analyzed for sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "%run ~/twitter_credentials.py\n",
    "\n",
    "#Use tweepy.OAuthHandler to create an authentication using the given key and secret\n",
    "auth = tweepy.OAuthHandler(consumer_key=con_key, consumer_secret=con_secret)\n",
    "auth.set_access_token(acc_token, acc_secret)\n",
    "\n",
    "#Connect to the Twitter API using the authentication\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "def get_tweets(geo_code, time_period, num_needed = 30000):\n",
    "    tweet_list = [] # create container for tweets\n",
    "    last_id = -1 # create index for last tweet seen\n",
    "    if time_period == 'before':\n",
    "        try: # see if before_tweets already exist\n",
    "            before_tweets\n",
    "        except: \n",
    "            pass # ignore else statement and use empty container and initial last_id index\n",
    "        else: # if it tweets does exist, set tweet list to tweets and index at last id of the list\n",
    "            tweet_list = before_tweets \n",
    "            last_id = before_tweets[-1].id\n",
    "        while len(tweet_list) < num_needed:\n",
    "            try:\n",
    "                new_tweets = api.search(q = '-filter:retweets', lang = 'en', geocode = geo_code, count = 100, max_id = str(last_id - 1), until = '2020-05-04', tweet_mode = 'extended')\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"Error\", e)\n",
    "                break\n",
    "            else:\n",
    "                if not new_tweets:\n",
    "                    print(\"Could not find any more tweets!\")\n",
    "                    break\n",
    "                tweet_list.extend(new_tweets)\n",
    "                last_id = new_tweets[-1].id\n",
    "    elif time_period == 'after':\n",
    "        try: # see if after_tweets already exists\n",
    "            after_tweets\n",
    "        except: \n",
    "            pass # ignore else statement and use empty container and initial last_id index\n",
    "        else: # if after_tweets does exist, set tweet list to tweets and index at last id of the list\n",
    "            tweet_list = after_tweets \n",
    "            last_id = after_tweets[-1].id\n",
    "        while len(tweet_list) < num_needed:\n",
    "            try:\n",
    "                # Change code to collect from the last tweet of the before_tweets (at 11:59:58, May 3rd)\n",
    "                new_tweets = api.search(q = '-filter:retweets', lang = 'en', geocode = geo_code, count = 100, max_id = str(last_id - 1), since_id = 1257097599406018573, tweet_mode = 'extended')\n",
    "            except tweepy.TweepError as e:\n",
    "                print(\"Error\", e)\n",
    "                break\n",
    "            else:\n",
    "                if not new_tweets:\n",
    "                    print(\"Could not find any more tweets!\")\n",
    "                    break\n",
    "                tweet_list.extend(new_tweets)\n",
    "                last_id = new_tweets[-1].id\n",
    "    return(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>> IMPORTANT <<<<<<<<<<<<< #\n",
    "# Switch following lines to 'True' to gather tweets and overwrite previously saved tweet data\n",
    "need_to_collect_save = False\n",
    "\n",
    "# >>>>>>>>>>> ARE YOU SURE? <<<<<<<<<<< #\n",
    "need_to_collect_save = False\n",
    "\n",
    "if need_to_collect_save == True:\n",
    "    tweets = get_tweets(geo_code = '37.6872,-97.3301,50km', time_period = 'before')\n",
    "    before_tweets = tweets\n",
    "\n",
    "    tweets = get_tweets(geo_code = '37.6872,-97.3301,50km', time_period = 'after')\n",
    "    after_tweets = tweets\n",
    "\n",
    "    to_save_before = [(tweet.id, tweet.user.screen_name, tweet.full_text, tweet.created_at) for tweet in before_tweets]\n",
    "    to_save_before = pd.DataFrame(to_save_before, columns = ['id', 'screen_name', 'text', 'date/time'])\n",
    "    to_save_before.to_csv('before_tweets.csv', index = False)\n",
    "\n",
    "    to_save_after = [(tweet.id, tweet.user.screen_name, tweet.full_text, tweet.created_at) for tweet in after_tweets]\n",
    "    to_save_after = pd.DataFrame(to_save_after, columns = ['id', 'screen_name', 'text', 'date/time'])\n",
    "    to_save_after.to_csv('after_tweets.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean collected tweets and tag sentiment values with TextBlob and custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load saved tweets\n",
    "before_tweets = pd.read_csv('before_tweets.csv')\n",
    "after_tweets = pd.read_csv('after_tweets.csv')\n",
    "\n",
    "# Clean tweet text\n",
    "before_tweets.text = before_tweets.text.apply(clean_tweet)\n",
    "after_tweets.text = after_tweets.text.apply(clean_tweet)\n",
    "\n",
    "# Save lists of before and after texts to feed to our model\n",
    "to_pred_before = before_tweets.text\n",
    "to_pred_after = after_tweets.text\n",
    "\n",
    "# Initialize 2 Tfidf vectorizers with given parameters\n",
    "tvec_before = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "tvec_after = TfidfVectorizer(max_features=100000,ngram_range=(1, 3))\n",
    "\n",
    "# Convert words in corpus to numbers, and store in vectorizers for use in the transform\n",
    "tvec_before.fit(to_pred_before)\n",
    "tvec_after.fit(to_pred_after)\n",
    "\n",
    "# Transform corpuses (corpi?) by mapping numbers in tfidf.vocabulary_ to tf-idf scores for each tweet\n",
    "# each object has a shape of (x, 100000), where x is the number of tweets\n",
    "before_tfidf = tvec_before.transform(to_pred_before)\n",
    "after_tfidf = tvec_after.transform(to_pred_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_sentiment(tweet): \n",
    "    # Create textblob object for tweet \n",
    "    analysis = TextBlob(tweet) \n",
    "    \n",
    "    # return sentiment, don't bin them into positive, neutral, or negative\n",
    "    return(analysis.sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# Load saved model structure\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "\n",
    "# Load model and saved weights\n",
    "model = model_from_json(loaded_model_json)\n",
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "custom_before = model.predict(before_tfidf)\n",
    "custom_after = model.predict(after_tfidf)\n",
    "\n",
    "textblob_before = []\n",
    "for i in to_pred_before:\n",
    "    textblob_before.append(get_tweet_sentiment(i))\n",
    "    \n",
    "textblob_after = []\n",
    "for j in to_pred_after:\n",
    "    textblob_after.append(get_tweet_sentiment(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns containing prediction values from custom model as well as TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tweets['custom prediction'] = custom_before\n",
    "after_tweets['custom prediction'] = custom_after\n",
    "    \n",
    "before_tweets['textblob prediction'] = textblob_before\n",
    "after_tweets['textblob prediction'] = textblob_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_tweets.to_csv('before_r.csv', index = False)\n",
    "after_tweets.to_csv('after_r.csv', index = False)\n",
    "\n",
    "# Save culled version of data for submission\n",
    "before_tweets[0:500].to_csv('culled_before_r.csv', index = False)\n",
    "after_tweets[0:500].to_csv('culled_after_r.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
